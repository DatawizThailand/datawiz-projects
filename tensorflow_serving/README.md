# TensorFlow Serving
> Tutorial ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á Medium blog post ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö https://medium.com/@poom.wettayakorn/https-medium-com-poom-wettayakorn-deploy-image-recognition-using-tensorflow-serving-253f210f982e

TensorFlow Serving ‡∏Ñ‡∏∑‡∏≠ High Performance Serving System ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô Production

‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏• inference ‡∏Ç‡∏≠‡∏á machine learning ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ö‡∏ô scale ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡∏°‡∏µ features ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö use cases ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:

* ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô (‡∏ä‡πà‡∏ß‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ experiments ‡πÅ‡∏•‡∏∞ A/B testing)

* ‡∏õ‡∏•‡πà‡∏≠‡∏¢ API endpoints ‡πÉ‡∏´‡πâ‡∏ù‡∏±‡πà‡∏á client (‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á RPC ‡πÅ‡∏•‡∏∞ HTTP protocol)

* ‡∏•‡∏î latency ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£ GPU ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

## Serve a Tensorflow model in 5 minutes

### ResNet
‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á folder ‡πÑ‡∏ß‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• /tmp/resnet ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î resnet_v2_fp32_savedmodel ‡∏î‡πâ‡∏ß‡∏¢ cURL ‡πÅ‡∏•‡∏∞ extract ‡πÑ‡∏ü‡∏•‡πå

```bash
$ mkdir /tmp/resnet
$ curl -s https://storage.googleapis.com/download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NHWC_jpg.tar.gz | tar --strip-components=2 -C /tmp/resnet -xvz
```

### TensorFlow Serving with¬†Docker
‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ Pre-trained ResNet ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏±‡∏ô TensorFlow Serving server ‡πÇ‡∏î‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô docker ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏Ñ‡πà‡∏£‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏Å‡∏µ‡πà commands ‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á server ‡πÅ‡∏•‡∏∞ API endpoints ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

```bash
$ docker pull tensorflow/serving
$ docker run --rm -it -p 8501:8501 -v /tmp/resnet:/models/resnet -e MODEL_NAME=resnet tensorflow/serving
```

Docker run command ‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á:

* `-p 8501:8501`¬†: ‡πÄ‡∏õ‡∏¥‡∏î‡∏û‡∏≠‡∏£‡πå‡∏ï 8501 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö REST API (8500 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö gRPC)
* `-v /tmp/resnet:/models/resnet`¬†: mount directory ‡∏Ç‡∏≠‡∏á local (/tmp/resnet) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà container ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠ /models/resnet
* `-e MODEL_NAME=resnet`¬†: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ä‡∏∑‡πà‡∏≠ "resnet"

### Model Inference

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏• Inference ‡πÇ‡∏î‡∏¢‡∏£‡∏±‡∏ô client ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å REST API ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Serving server

```bash
$ curl -o /tmp/resnet/resnet_client.py https://raw.githubusercontent.com/tensorflow/serving/master/tensorflow_serving/example/resnet_client.py
$ python /tmp/resnet/resnet_client.py
```
‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå prediction ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤ average latency ‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 59 ms üëç

## Reference
* [https://github.com/tensorflow/serving](https://github.com/tensorflow/serving)
* [https://www.tensorflow.org/serving](https://www.tensorflow.org/serving)
